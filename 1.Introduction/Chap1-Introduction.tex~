\section{Introduction}
\label{sec:introduction}
Most neurons in the brain interact with each other by generating an action potential (AP), a fast,
transient, and stereotypical fluctuation in the membrane potential of the cell, commonly referredto as a spike. The AP propagates along the neuron following a consistent trajectory from the soma (the neuron's body) through the axon to the synapse. 

Between the neuron and the extracellular medium there is a voltage difference called the membrane potential. When at rest, the membrane potential is negative, around -70mV.  In the membrane there are voltage-gated ion channel which molecular structure that react to the value of the membrane potential. Synaptic inputs from neurons upstream cause the membrane potential to vary. When the membrane potential exceeds a certain threshold the ion channels open allowing ions (such Na+ and K+) to flow in and out the neuron. This causes an abrupt change in the membrane potential called the action potential (AP) (usually refered to as spikes). The AP is a fast, transient, and stereotypical fluctuation in the membrane potential of for each cell, commonly referred to as a spike. The AP propagates along the neuron following a consistent trajectory from the soma (the neuron's body) through the axon to the synapse. 

Consequently, as ions flow during the propagation of the AP they cause a disturbance in the charge distribution in the extracellular medium, producing the Extracellular Action Potential (EAP). The EAP propagates outwards in the extracellular medium. \cite{kandel}

while intracellular action potentials AP are so stereotyped, the EAPs waveform show a large variability. Not only morphologic aspects of the neuron influence the characteristics of the EAP, but as the AP is propagated intracellularly there is a continuous generation of EAPs  down the axon. This leads a complex propagation of the EAP through the extracellular medium. [Gold -  thesis]
 
To study the dynamics of neural activity, electrophysiology is still the most widely used, in particular, extracellular recording, which measures the voltage fluctuations that surround the electrodes, including the EAPs generated relatively close to the electrode.
 
The signal is acquired and then, usually offline, the data is processed and spike detection is performed, where researcher try to find when a spike took place. Then the detected spikes are assigned to one neuron, through a process called spike sorting.

At first, using single sharp electrodes, researchers were able to detect and sort reliably the activity of one or two neurons in the vicinity of each electrode. Using tetrode electrodes (four electrodes fairly close to each other), it is possible to isolate up to 20 neurons (\cite{mcnaughton1983stereotrode}, \cite{gray1995tetrodes}, \cite{wilson1993dynamics}, \cite{recce1989tetrode}). This increase is understandable. Due to the complexity of the propagation of the EAPs different neurons have not only different firing times but also different set of waveforms acquired by the various electrodes. These spatiotemporal profiles, dependent on the morphology, position and orientation to firing neuron, can be use to further sort the detected spikes.  

This led neuroscientist to seek for probes with more and more electrodes. Indeed, advances in microfabrication made it possible to produce probes with hundreds of electrodes densely positioned along large distances (~500um) (references for probes). And Employing modern methods for integrated circuit design and fabrication, probe with thousands or even million, of discrete sites are being developed (\cite{dombovari2014vivo}, \cite{ruther2015new}, \cite{shobe2015brain})

For tetrode data, it is possible to achieve sorting error rates of 5\% or lower (\cite{harris2000accuracy}). However, the algorithms that performed fairly well on data from tetrodes do not work on the these probes. This happens due to the high dimensionality of the data: "the curse of dimensionality" greatly affects the performance of the automated part of the algorithm, and makes the manual inspection much harder.

While many different methods for spike sorting have been proposed, no method is robust enough to be widely adopted by the experimental community.

Furthermore, since these new generation probes are larger, they are prone to sensing temporally overlapping spikes, which doesn't happen very often with tetrodes. However, temporally overlapping spikes usually can be resolved in the spatial domain: each spike spans over a different  set of electrodes (usually called the neuron's footprint).

In Neto et al., they perform paired recording with a juxtacellular pipette and new generation silicon probes with 32 and 128 electrodes. With this dataset we have a precise determination of when one neuron was active. With this information it is possible to compute triggered averages allowing for the study of the propagation of the EAP.

Also, with this dataset it is possible to evaluate the performance and limitations of spike detection and spike sorting algorithms. In particular, in Rossant et al., a method was developed that uses the information about the relative position of electrodes in a multi-electrode array in order to take advantage of the “neuron footprints”.

On the other hand, these paired recordings can be seen as labeled datasets where each portion of the extracellular recording is assigned a classification regarding whether or not it contains an EAP from the neuron the juxtacellular probe was recording from. This seems a suitable setup for uses of machine learning techniques, in particular, supervised learning.

Machine Learning is a subfield of computer science that studies and develops algorithms meant to find new structures or rules a given experiment or phenomenon is based on. In machine learning, a computer receives a set of examples of inputs and it tries to determine the dependence between them. For example, given as examples the pair height and weight of many people, the computer tries to determine the relation between one and the other. 
 
Moreover, it often desirable that the relation the computer found to be generalizable to region of the input space where no example lied. If this is accomplished it is said that computer found a predictor. In other words, scientists want to be able to predict the right output of the phenomenon under study in situation that weren't considered in the set of examples. 
When, for each of these input examples, there exist a “right answer” the model should output, it is a case of supervised learning.

When applying machine learning, a model or an heuristic must be chosen a priori. This assumption conditions the success of the learning. One such model can be a linear dependence between inputs and outputs. However, in many situations, this model is too simple to yield satisfactory results. In particular in the case of classification task, the boundary in the input space that separates the  classes may not be a plane, and therefore the linear model is not a good classifier.

For this reason, Artificial Neural Networks were invented. These are a family of models inspired by the biological neural networks in the brain. They consist of connecting many simple computational units called artificial neurons, that together may yield very complex computation. Given this framework, a model is defined by determining how the artificial neurons are connected. 

ANNs managed to solve some problems that couldn't be solved before, such as classification problem that weren't linearly separated.

When determining the model in the ANN framework, it is necessary to choose its architecture: how many artificial neuron, how many layer, how many neurons per layer and how they are connected to each other.  However, the training of ANN was very complicated until the advent the backpropagation method for computation of gradients in 1970s.  But even after this, deep neural networks (ANNs with more than 3 layers) would suffer from the problem of the “vanishing gradients”, making the training extremely slow. (\cite{hochreiter2001gradient})

Even if shallow neural networks (ANNs with at most 3 layers) can be used to learn many difficult task, deep neural networks (DNN) were always theoretically more appealing since in each layer lies a new more abstract representation of the input layer, and therefore could be more “human-like” (\cite{rumelhart1988learning})

However, only in 2006 could DNN be properly training due to discoveries on initialization methods (\cite{hinton2006fast}, Greedy Layer-Wise Training of Deep Networks) and the use of GPU accelerated algorithms (Large-scale Deep Unsupervised Learning using Graphics Processors)
Since then, deep neural networks have been used to task such as image and speech recognition and in some cases achieving “superhuman” levels of accuracy.

In the present document, I report the attempt of applying methods from the deep learning to the task of spike detection. In Chapter 2, I will present in detail the dataset from Neto et al. And how it was acquired. I will present the results of applying SpikeDetekt to the dataset and assert its accuracy. In Chapter 3, I first define and explain some of methods of machine learning and deep learning and then the result of implementation of such algorithms to generate a spike detection method for multi-electrodes array. Finally I compare it to the results from SpikeDetekt

